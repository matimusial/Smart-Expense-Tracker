Plan 1:
BERT_EPOCHS: 3
BERT_BATCH_SIZE: 16
BERT_LEARNING_RATE: 3e-5
BERT_TEST_SIZE: 0.16
BERT_MODEL = 'allegro/herbert-large-cased'

Wyniki:

Confusion Matrix:
[[ 42   0   0   2   0   0   0   0   0   0   1   1   0   0   5   0   0   0]
 [  0  52   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   2  37   0   0   0   0   0   0   1   0   0   0   0   0   0   0   0]
 [  0   0   0 107   2   0   0   0   0   0   1   0   0   0   3   2   0   1]
 [  0   0   0   1  68   0   0   0   0   0   5   1   0   0   2   4   0   0]
 [  0   1   0   0   0  22   1   0   0   0   0   0   0   2   0   0   0   0]
 [  0   1   0   0   1   5  74   0   0   0   0   1   0   4   0   0   0   0]
 [  0   0   0   0   0   0   0  36   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   1   0   0   0   0  51   0   2   0   0   2   3   1   1   0]
 [  0   0   2   0   0   0   0   0   0  65   0   0   0   0   0   0   0   0]
 [  3   4   0   1   1   0   1   0   1   0 129   0   0   1   4   0   0   1]
 [  2   4   0   1   0   0   0   0   0   0   2  86   0   1   3   0   2   0]
 [  0   0   0   0   0   0   0   0   1   0   0   0 112   0   0   0   0   0]
 [  0   2   3   0   0   0   0   0   0   1   0   1   0  67   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   2   0   0  38   1   1   2]
 [  0   0   0   0   2   0   2   0   0   0   3   0   0   0   3 110   0   0]
 [  0   0   0   0   0   0   0   0   0   0   1   0   3   0   0   0  93   1]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   1   0   0  21]]

Classification Report:
              precision    recall  f1-score   support

           0       0.89      0.82      0.86        51
           1       0.79      1.00      0.88        52
           2       0.88      0.93      0.90        40
           3       0.95      0.92      0.93       116
           4       0.92      0.84      0.88        81
           5       0.81      0.85      0.83        26
           6       0.95      0.86      0.90        86
           7       1.00      1.00      1.00        36
           8       0.96      0.84      0.89        61
           9       0.97      0.97      0.97        67
          10       0.90      0.88      0.89       146
          11       0.93      0.85      0.89       101
          12       0.97      0.99      0.98       113
          13       0.87      0.91      0.89        74
          14       0.61      0.86      0.72        44
          15       0.93      0.92      0.92       120
          16       0.96      0.95      0.95        98
          17       0.81      0.95      0.88        22

    accuracy                           0.91      1334
   macro avg       0.90      0.91      0.90      1334
weighted avg       0.91      0.91      0.91      1334


Precision (Weighted): 0.9136
Recall (Weighted): 0.9070
F1-Score (Weighted): 0.9083
84/84 [==============================] - 8s 50ms/step - loss: 0.3299 - accuracy: 0.9070
Test Loss: 0.3299, Test Accuracy: 0.9070


Plan 2:
BERT_EPOCHS: 4
BERT_BATCH_SIZE: 16
BERT_LEARNING_RATE: 3e-5
BERT_TEST_SIZE: 0.10
BERT_MODEL = 'allegro/herbert-large-cased'

Wyniki:

Confusion Matrix:
[[31  0  0  0  0  0  0  0  0  0  0  0  0  0  1  0  0  0]
 [ 0 31  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0 23  0  1  1  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0 72  0  0  0  0  0  0  0  0  0  0  1  0  0  0]
 [ 0  0  0  0 44  0  0  0  1  0  1  1  1  0  0  1  1  0]
 [ 0  0  0  0  0 14  2  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0 52  0  0  0  0  0  0  0  0  2  0  0]
 [ 0  0  0  0  0  0  0 23  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  1  0  0  0  0 36  0  0  0  0  0  0  0  0  0]
 [ 0  0  2  0  0  0  0  1  0 39  0  0  0  0  0  0  0  0]
 [ 1  0  0  1  1  0  3  0  1  0 78  0  1  1  0  1  1  2]
 [ 0  0  0  0  0  0  0  0  0  0  1 61  0  1  0  0  0  0]
 [ 0  0  0  0  0  1  0  0  0  0  0  0 67  0  0  0  3  0]
 [ 0  0  3  1  0  1  2  0  0  0  0  0  0 39  0  0  0  0]
 [ 1  0  0  0  0  0  0  0  1  0  0  0  0  0 23  2  0  1]
 [ 0  0  0  0  2  0  0  0  0  0  0  1  0  0  1 71  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  3 59  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  2  0  0 12]]

Classification Report:
              precision    recall  f1-score   support

           0       0.94      0.97      0.95        32
           1       1.00      0.97      0.98        32
           2       0.82      0.92      0.87        25
           3       0.96      0.99      0.97        73
           4       0.92      0.88      0.90        50
           5       0.78      0.88      0.82        16
           6       0.88      0.96      0.92        54
           7       0.96      1.00      0.98        23
           8       0.92      0.97      0.95        37
           9       1.00      0.93      0.96        42
          10       0.97      0.86      0.91        91
          11       0.97      0.97      0.97        63
          12       0.97      0.94      0.96        71
          13       0.95      0.85      0.90        46
          14       0.82      0.82      0.82        28
          15       0.89      0.95      0.92        75
          16       0.92      0.95      0.94        62
          17       0.80      0.86      0.83        14

    accuracy                           0.93       834
   macro avg       0.92      0.93      0.92       834
weighted avg       0.93      0.93      0.93       834


Precision (Weighted): 0.9318
Recall (Weighted): 0.9293
F1-Score (Weighted): 0.9294
Test Loss: 0.3129, Test Accuracy: 0.9293




Plan 3:
BERT_EPOCHS: 2
BERT_BATCH_SIZE: 16
BERT_LEARNING_RATE: 2e-5
BERT_TEST_SIZE: 0.20
BERT_MODEL = 'allegro/herbert-large-cased'

Wyniki:

Confusion Matrix:
[[ 57   0   0   0   0   0   0   0   0   0   0   0   0   0   7   0   0   0]
 [  0  64   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0  39   0   0   1   0   0   0   9   0   0   0   1   0   0   0   0]
 [  1   0   0 136   0   0   0   0   3   0   0   0   0   0   4   1   0   0]
 [  0   0   0   0  97   0   0   0   0   0   0   0   0   0   2   1   1   0]
 [  0   0   1   0   0  29   1   0   0   0   0   0   0   1   0   0   0   0]
 [  0   0   0   0   0   3 100   0   0   0   3   0   0   2   0   0   0   0]
 [  0   0   0   0   0   0   0  45   0   0   0   0   0   0   0   0   0   0]
 [  0   1   0   2   0   0   0   0  66   0   3   0   0   0   1   1   1   0]
 [  0   0   1   0   0   0   0   1   0  82   0   0   0   0   0   0   0   0]
 [  1   0   0   4   5   1   3   0   5   0 149   5   0   1   4   2   1   2]
 [  1   0   0   1   4   0   0   0   0   0   0 115   0   1   4   1   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 140   0   0   0   0   1]
 [  0   0   2   0   0   0   4   0   2   3   0   0   1  79   0   0   1   0]
 [  1   0   0   0   0   0   0   0   0   0   0   0   1   0  50   2   1   0]
 [  0   0   0   0  11   0   0   0   0   0   0   0   0   0   7 128   3   1]
 [  0   0   0   0   2   0   0   0   0   0   0   0   3   0   3   4 110   1]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   2   0   0  26]]

Classification Report:
              precision    recall  f1-score   support

           0       0.93      0.89      0.91        64
           1       0.98      1.00      0.99        64
           2       0.91      0.78      0.84        50
           3       0.95      0.94      0.94       145
           4       0.82      0.96      0.88       101
           5       0.85      0.91      0.88        32
           6       0.93      0.93      0.93       108
           7       0.98      1.00      0.99        45
           8       0.87      0.88      0.87        75
           9       0.87      0.98      0.92        84
          10       0.96      0.81      0.88       183
          11       0.96      0.91      0.93       127
          12       0.97      0.99      0.98       141
          13       0.93      0.86      0.89        92
          14       0.60      0.91      0.72        55
          15       0.91      0.85      0.88       150
          16       0.93      0.89      0.91       123
          17       0.84      0.93      0.88        28

    accuracy                           0.91      1667
   macro avg       0.90      0.91      0.90      1667
weighted avg       0.92      0.91      0.91      1667


Precision (Weighted): 0.9151
Recall (Weighted): 0.9070
F1-Score (Weighted): 0.9083
Test Loss: 0.3407, Test Accuracy: 0.9070



Plan 4:
BERT_EPOCHS: 3
BERT_BATCH_SIZE: 32
BERT_LEARNING_RATE: 3e-5
BERT_TEST_SIZE: 0.25
BERT_MODEL = 'allegro/herbert-large-cased'

Wyniki:

Confusion Matrix:
[[ 77   0   0   0   0   0   0   0   0   0   2   0   0   0   1   0   0   0]
 [  0  79   0   0   0   0   1   1   0   0   0   0   0   0   0   0   0   0]
 [  0   0  60   0   0   2   0   0   0   1   0   0   0   0   0   0   0   0]
 [  0   0   0 149   1   2   0   0  21   0   1   3   0   1   1   2   0   1]
 [  0   0   0   1 111   0   0   0   0   0   1   2   0   0   0   8   3   0]
 [  0   0   0   0   0  37   3   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   2 127   0   0   0   2   0   0   2   0   1   0   0]
 [  0   0   0   0   0   1   0  56   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   1   0   0   0   0  90   0   1   0   0   0   1   1   0   0]
 [  0   0   3   0   0   0   0   1   0 101   0   0   0   0   0   0   0   0]
 [  2   1   0   1   0   0   3   0   2   0 200   3   0   2   3   8   2   1]
 [  3   0   0   0   1   0   0   0   0   0   0 149   2   0   1   2   0   0]
 [  0   0   0   0   0   0   1   0   0   0   1   0 174   0   0   0   0   0]
 [  0   0   4   1   0  10   6   0   0   0   0   0   0  94   0   0   0   0]
 [  3   0   0   1   0   0   0   0   2   0   0   0   1   0  55   7   0   0]
 [  0   1   0   0   6   0   0   0   1   0   2   0   1   0   5 166   5   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   1   1   0   3 148   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   1   0   0   2   0  32]]

Classification Report:
              precision    recall  f1-score   support

           0       0.91      0.96      0.93        80
           1       0.98      0.98      0.98        81
           2       0.90      0.95      0.92        63
           3       0.97      0.82      0.89       182
           4       0.93      0.88      0.91       126
           5       0.69      0.93      0.79        40
           6       0.90      0.95      0.92       134
           7       0.97      0.98      0.97        57
           8       0.78      0.96      0.86        94
           9       0.99      0.96      0.98       105
          10       0.95      0.88      0.91       228
          11       0.95      0.94      0.95       158
          12       0.97      0.99      0.98       176
          13       0.94      0.82      0.87       115
          14       0.82      0.80      0.81        69
          15       0.83      0.89      0.86       187
          16       0.94      0.97      0.95       153
          17       0.94      0.91      0.93        35

    accuracy                           0.91      2083
   macro avg       0.91      0.92      0.91      2083
weighted avg       0.92      0.91      0.91      2083


Precision (Weighted): 0.9193
Recall (Weighted): 0.9145
F1-Score (Weighted): 0.9149
Test Loss: 0.2991, Test Accuracy: 0.9145




Plan 5:
BERT_EPOCHS: 3
BERT_BATCH_SIZE: 8
BERT_LEARNING_RATE: 1e-5
BERT_TEST_SIZE: 0.16
BERT_MODEL = 'allegro/herbert-large-cased'

Wyniki:


Confusion Matrix:
[[ 49   0   0   1   0   0   0   0   0   0   0   0   0   0   1   0   0   0]
 [  0  51   0   0   0   0   0   0   0   0   0   0   1   0   0   0   0   0]
 [  0   0  33   0   0   4   0   0   0   2   0   0   0   1   0   0   0   0]
 [  0   0   0 108   0   0   0   0   6   0   1   0   0   1   0   0   0   0]
 [  0   0   0   0  75   0   0   0   0   0   1   1   1   0   0   2   1   0]
 [  0   0   0   0   0  20   1   1   0   0   1   0   0   3   0   0   0   0]
 [  0   0   0   0   0   3  80   0   0   0   1   0   0   2   0   0   0   0]
 [  0   0   0   0   0   0   0  36   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0  57   0   2   0   0   0   1   1   0   0]
 [  0   0   0   0   0   0   0   0   0  67   0   0   0   0   0   0   0   0]
 [  0   0   0   2   3   0   4   0   2   0 132   0   0   0   0   0   3   0]
 [  1   0   0   0   0   0   0   0   0   0   0  94   0   0   4   0   2   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 112   0   0   0   1   0]
 [  0   1   1   0   0   1   3   0   0   0   1   0   0  67   0   0   0   0]
 [  1   0   0   0   0   0   0   0   1   0   0   0   1   0  37   2   0   2]
 [  0   0   0   0   3   0   0   0   0   0   4   1   0   0   3 107   2   0]
 [  1   0   0   0   0   0   0   0   0   0   0   1   1   0   0   2  93   0]
 [  1   0   0   1   0   0   0   0   0   0   0   0   0   0   0   0   1  19]]

Classification Report:
              precision    recall  f1-score   support

           0       0.92      0.96      0.94        51
           1       0.98      0.98      0.98        52
           2       0.97      0.82      0.89        40
           3       0.96      0.93      0.95       116
           4       0.93      0.93      0.93        81
           5       0.71      0.77      0.74        26
           6       0.91      0.93      0.92        86
           7       0.97      1.00      0.99        36
           8       0.86      0.93      0.90        61
           9       0.97      1.00      0.99        67
          10       0.92      0.90      0.91       146
          11       0.97      0.93      0.95       101
          12       0.97      0.99      0.98       113
          13       0.91      0.91      0.91        74
          14       0.80      0.84      0.82        44
          15       0.94      0.89      0.91       120
          16       0.90      0.95      0.93        98
          17       0.90      0.86      0.88        22

    accuracy                           0.93      1334
   macro avg       0.92      0.92      0.92      1334
weighted avg       0.93      0.93      0.93      1334


Precision (Weighted): 0.9284
Recall (Weighted): 0.9273
F1-Score (Weighted): 0.9274
Test Loss: 0.2860, Test Accuracy: 0.9273



Plan 6: - saved
BERT_EPOCHS: 2
BERT_BATCH_SIZE: 16
BERT_LEARNING_RATE: 3e-5
BERT_TEST_SIZE: 0.16
BERT_MODEL = 'allegro/herbert-large-cased'

Wyniki:

Confusion Matrix:
[[ 46   0   0   0   0   0   0   0   0   0   2   0   0   0   3   0   0   0]
 [  0  51   0   0   0   0   0   1   0   0   0   0   0   0   0   0   0   0]
 [  0   0  33   0   0   0   0   2   0   4   0   0   0   1   0   0   0   0]
 [  0   0   0 103   3   0   0   0   0   0   2   0   1   1   6   0   0   0]
 [  0   0   0   0  74   0   0   0   0   0   1   0   0   1   0   3   2   0]
 [  0   0   0   0   0  23   2   0   0   0   0   0   0   1   0   0   0   0]
 [  0   0   0   0   0   2  80   0   0   0   1   0   0   2   0   1   0   0]
 [  0   0   0   0   0   0   0  36   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   6   0   0   0   0  50   0   3   0   0   0   2   0   0   0]
 [  0   0   1   0   0   0   0   0   0  66   0   0   0   0   0   0   0   0]
 [  6   0   0   0   3   0   1   0   2   0 126   2   0   0   0   4   0   2]
 [  2   0   0   0   0   0   0   0   0   0   1  92   0   1   4   1   0   0]
 [  0   0   0   0   0   0   0   0   0   0   1   1 111   0   0   0   0   0]
 [  0   0   0   0   0   1   2   0   1   1   0   0   0  65   0   3   1   0]
 [  4   0   0   0   0   0   0   0   0   0   0   0   1   0  39   0   0   0]
 [  0   0   0   0   5   0   1   0   0   0   5   1   0   0   9  97   2   0]
 [  1   0   0   0   3   0   0   0   0   0   0   0   0   0   0   0  94   0]
 [  0   1   0   0   0   0   0   0   0   0   0   0   0   0   1   1   0  19]]

Classification Report:
              precision    recall  f1-score   support

           0       0.78      0.90      0.84        51
           1       0.98      0.98      0.98        52
           2       0.97      0.82      0.89        40
           3       0.94      0.89      0.92       116
           4       0.84      0.91      0.88        81
           5       0.88      0.88      0.88        26
           6       0.93      0.93      0.93        86
           7       0.92      1.00      0.96        36
           8       0.94      0.82      0.88        61
           9       0.93      0.99      0.96        67
          10       0.89      0.86      0.88       146
          11       0.96      0.91      0.93       101
          12       0.98      0.98      0.98       113
          13       0.90      0.88      0.89        74
          14       0.61      0.89      0.72        44
          15       0.88      0.81      0.84       120
          16       0.95      0.96      0.95        98
          17       0.90      0.86      0.88        22

    accuracy                           0.90      1334
   macro avg       0.90      0.90      0.90      1334
weighted avg       0.91      0.90      0.90      1334


Precision (Weighted): 0.9094
Recall (Weighted): 0.9033
F1-Score (Weighted): 0.9045
Test Loss: 0.3305, Test Accuracy: 0.9033




Plan 7:
BERT_EPOCHS: 3
BERT_BATCH_SIZE: 16
BERT_LEARNING_RATE: 2e-5
BERT_TEST_SIZE: 0.16
BERT_MODEL = 'allegro/herbert-base-cased'

Wyniki:

Confusion Matrix:
[[ 49   0   0   0   1   0   0   0   0   0   0   0   0   0   1   0   0   0]
 [  0  51   0   0   0   0   0   1   0   0   0   0   0   0   0   0   0   0]
 [  0   0  39   0   0   0   0   0   0   0   0   0   0   1   0   0   0   0]
 [  2   1   0  95   2   0   0   0  12   0   0   0   0   0   2   1   1   0]
 [  0   0   0   0  75   0   0   0   0   0   2   0   0   0   0   1   3   0]
 [  0   0   0   0   0  26   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   4  79   0   0   0   3   0   0   0   0   0   0   0]
 [  0   0   0   0   0   1   0  34   0   1   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0  58   0   0   2   0   0   0   1   0   0]
 [  0   0   1   0   0   0   0   1   0  65   0   0   0   0   0   0   0   0]
 [  4   0   0   1   5   0   0   0   1   0 128   1   1   1   1   2   1   0]
 [  2   0   0   1   1   0   0   0   0   0   0  92   1   0   2   0   2   0]
 [  0   1   0   0   0   0   0   0   0   0   0   1 107   0   1   0   2   1]
 [  0   0   5   0   0   8   5   0   0   0   0   0   1  55   0   0   0   0]
 [  2   0   0   1   1   0   0   0   2   0   0   2   2   0  32   1   1   0]
 [  0   0   0   0   4   0   0   0   0   0   2   2   0   0   3 105   3   1]
 [  0   0   0   0   1   0   0   0   0   0   1   0   0   0   2   2  92   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   2   0   0  20]]

Classification Report:
              precision    recall  f1-score   support

           0       0.83      0.96      0.89        51
           1       0.96      0.98      0.97        52
           2       0.87      0.97      0.92        40
           3       0.97      0.82      0.89       116
           4       0.83      0.93      0.88        81
           5       0.67      1.00      0.80        26
           6       0.94      0.92      0.93        86
           7       0.94      0.94      0.94        36
           8       0.79      0.95      0.87        61
           9       0.98      0.97      0.98        67
          10       0.94      0.88      0.91       146
          11       0.92      0.91      0.92       101
          12       0.96      0.95      0.95       113
          13       0.96      0.74      0.84        74
          14       0.70      0.73      0.71        44
          15       0.93      0.88      0.90       120
          16       0.88      0.94      0.91        98
          17       0.91      0.91      0.91        22

    accuracy                           0.90      1334
   macro avg       0.89      0.91      0.89      1334
weighted avg       0.91      0.90      0.90      1334


Precision (Weighted): 0.9081
Recall (Weighted): 0.9010
F1-Score (Weighted): 0.9015
Test Loss: 0.3106, Test Accuracy: 0.9010



Plan 8:
BERT_EPOCHS: 2
BERT_BATCH_SIZE: 32
BERT_LEARNING_RATE: 3e-5
BERT_TEST_SIZE: 0.16
BERT_MODEL = 'allegro/herbert-base-cased'

Wyniki:

Confusion Matrix:
[[ 48   0   0   0   0   0   0   0   0   0   0   0   0   0   3   0   0   0]
 [  0  49   0   0   0   1   0   2   0   0   0   0   0   0   0   0   0   0]
 [  0   0  37   0   0   2   0   0   0   1   0   0   0   0   0   0   0   0]
 [  0   1   0 109   1   0   0   0   3   0   0   1   0   0   1   0   0   0]
 [  0   0   0   0  73   0   0   0   0   0   2   1   0   0   0   0   2   3]
 [  0   0   0   0   0  22   2   2   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   1  83   0   0   0   0   0   0   1   0   1   0   0]
 [  0   0   0   0   0   0   0  36   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   2   1   0   0   0  53   0   3   1   0   0   1   0   0   0]
 [  0   0   0   0   0   0   0   2   0  65   0   0   0   0   0   0   0   0]
 [  2   2   0   4   3   0   3   0   2   0 116   3   0   0   2   8   1   0]
 [  1   0   0   2   0   0   0   0   0   0   0  95   2   0   0   1   0   0]
 [  0   0   0   1   0   0   0   0   0   0   0   0 110   0   0   0   1   1]
 [  0   0   1   0   0   1   7   0   0   2   0   0   0  63   0   0   0   0]
 [  2   0   0   0   1   0   0   0   0   0   0   3   2   0  33   3   0   0]
 [  0   0   0   0   2   0   1   0   0   0   0   0   0   0   4 109   3   1]
 [  0   0   0   0   0   0   0   0   1   0   0   0   0   0   0   3  94   0]
 [  0   0   0   0   0   0   0   0   1   0   0   1   0   0   0   1   0  19]]

Classification Report:
              precision    recall  f1-score   support

           0       0.91      0.94      0.92        51
           1       0.94      0.94      0.94        52
           2       0.97      0.93      0.95        40
           3       0.92      0.94      0.93       116
           4       0.90      0.90      0.90        81
           5       0.81      0.85      0.83        26
           6       0.86      0.97      0.91        86
           7       0.86      1.00      0.92        36
           8       0.88      0.87      0.88        61
           9       0.96      0.97      0.96        67
          10       0.96      0.79      0.87       146
          11       0.90      0.94      0.92       101
          12       0.96      0.97      0.97       113
          13       0.98      0.85      0.91        74
          14       0.75      0.75      0.75        44
          15       0.87      0.91      0.89       120
          16       0.93      0.96      0.94        98
          17       0.79      0.86      0.83        22

    accuracy                           0.91      1334
   macro avg       0.90      0.91      0.90      1334
weighted avg       0.91      0.91      0.91      1334


Precision (Weighted): 0.9125
Recall (Weighted): 0.9100
F1-Score (Weighted): 0.9097
Test Loss: 0.2898, Test Accuracy: 0.9100


Plan 9:
BERT_EPOCHS: 2
BERT_BATCH_SIZE: 16
BERT_LEARNING_RATE: 3e-5
BERT_TEST_SIZE: 0.2
BERT_MODEL = 'allegro/herbert-base-cased'

Wyniki:

Confusion Matrix:
[[ 61   0   0   0   0   0   0   0   0   0   0   1   0   0   2   0   0   0]
 [  0  61   0   0   0   0   0   0   0   0   0   1   0   0   0   0   2   0]
 [  0   0  48   0   0   0   0   0   0   0   0   1   0   1   0   0   0   0]
 [  0   0   0 134   0   0   0   0   4   0   1   0   0   1   3   2   0   0]
 [  0   0   0   0  94   0   0   0   0   0   2   0   0   0   0   3   2   0]
 [  0   0   0   0   0  29   1   0   0   0   0   0   0   2   0   0   0   0]
 [  0   0   0   0   0   3 104   0   0   0   0   1   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  45   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   3   0   0   0   0  70   0   1   0   0   1   0   0   0   0]
 [  0   0   5   0   0   0   0   0   0  79   0   0   0   0   0   0   0   0]
 [  2   0   0   1   3   0   6   0   6   0 152   4   2   1   1   3   1   1]
 [  2   0   0   2   1   0   0   0   0   0   2 116   1   0   1   2   0   0]
 [  0   0   0   0   0   0   0   0   0   0   1   0 138   0   0   0   1   1]
 [  0   0   0   0   0   1   4   0   1   0   0   1   0  85   0   0   0   0]
 [  4   0   0   1   0   0   0   0   2   0   2   2   0   0  36   7   1   0]
 [  0   0   0   0   6   0   1   0   0   0   1   7   0   0   1 131   3   0]
 [  0   0   0   0   2   0   0   0   0   0   2   2   3   0   2   5 107   0]
 [  0   0   0   0   1   0   0   0   0   0   0   0   0   0   1   1   0  25]]

Classification Report:
              precision    recall  f1-score   support

           0       0.88      0.95      0.92        64
           1       1.00      0.95      0.98        64
           2       0.91      0.96      0.93        50
           3       0.95      0.92      0.94       145
           4       0.88      0.93      0.90       101
           5       0.88      0.91      0.89        32
           6       0.90      0.96      0.93       108
           7       1.00      1.00      1.00        45
           8       0.84      0.93      0.89        75
           9       1.00      0.94      0.97        84
          10       0.93      0.83      0.88       183
          11       0.85      0.91      0.88       127
          12       0.96      0.98      0.97       141
          13       0.93      0.92      0.93        92
          14       0.77      0.65      0.71        55
          15       0.85      0.87      0.86       150
          16       0.91      0.87      0.89       123
          17       0.93      0.89      0.91        28

    accuracy                           0.91      1667
   macro avg       0.91      0.91      0.91      1667
weighted avg       0.91      0.91      0.91      1667


Precision (Weighted): 0.9099
Recall (Weighted): 0.9088
F1-Score (Weighted): 0.9084
Test Loss: 0.3208, Test Accuracy: 0.9088


Plan 10:
BERT_EPOCHS: 3
BERT_BATCH_SIZE: 32
BERT_LEARNING_RATE: 2e-5
BERT_TEST_SIZE: 0.15
BERT_MODEL = 'allegro/herbert-base-cased'

Wyniki:

Confusion Matrix:
[[ 47   0   0   0   0   0   0   0   0   0   1   0   0   0   0   0   0   0]
 [  0  48   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0  37   0   0   1   0   0   0   0   0   0   0   0   0   0   0   0]
 [  2   0   0  98   1   0   0   0   2   0   2   1   0   0   0   1   2   0]
 [  0   0   0   0  67   0   0   0   1   0   3   0   0   0   1   2   2   0]
 [  0   0   1   0   0  21   2   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   4  73   0   0   0   0   0   0   2   0   0   1   0]
 [  0   0   0   0   0   0   0  34   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0  53   0   3   1   0   0   0   0   0   0]
 [  0   0   2   0   0   1   0   0   0  60   0   0   0   0   0   0   0   0]
 [  2   0   0   0   2   0   3   0   0   0 125   1   0   0   2   2   0   0]
 [  1   0   0   1   0   0   0   0   0   0   1  84   0   1   5   2   0   0]
 [  0   0   0   0   0   1   0   0   0   0   0   1  98   0   0   1   4   0]
 [  0   0   6   1   0   1   1   0   0   0   0   0   0  59   0   0   1   0]
 [  4   0   0   0   1   0   0   0   0   0   0   1   0   0  34   2   0   0]
 [  0   0   0   0   2   0   0   0   0   0   2   0   0   0   6 102   0   0]
 [  0   0   0   0   1   0   0   0   0   0   1   1   0   0   0   5  84   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   2   1   0  18]]

Classification Report:
              precision    recall  f1-score   support

           0       0.84      0.98      0.90        48
           1       1.00      1.00      1.00        48
           2       0.80      0.97      0.88        38
           3       0.98      0.90      0.94       109
           4       0.91      0.88      0.89        76
           5       0.72      0.88      0.79        24
           6       0.92      0.91      0.92        80
           7       1.00      1.00      1.00        34
           8       0.95      0.93      0.94        57
           9       1.00      0.95      0.98        63
          10       0.91      0.91      0.91       137
          11       0.93      0.88      0.91        95
          12       1.00      0.93      0.97       105
          13       0.95      0.86      0.90        69
          14       0.68      0.81      0.74        42
          15       0.86      0.91      0.89       112
          16       0.89      0.91      0.90        92
          17       1.00      0.86      0.92        21

    accuracy                           0.91      1250
   macro avg       0.91      0.92      0.91      1250
weighted avg       0.92      0.91      0.91      1250


Precision (Weighted): 0.9190
Recall (Weighted): 0.9136
F1-Score (Weighted): 0.9149
Test Loss: 0.2917, Test Accuracy: 0.9136

Plan 11: - saved
BERT_EPOCHS: 3 
BERT_BATCH_SIZE: 16
BERT_LEARNING_RATE: 2e-5
BERT_TEST_SIZE: 0.16
BERT_MODEL = 'allegro/herbert-large-cased'

Wyniki:

Confusion Matrix:
[[ 48   0   0   0   0   0   0   0   0   0   1   0   0   0   2   0   0   0]
 [  0  52   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0  35   0   0   0   0   0   0   1   0   0   0   4   0   0   0   0]
 [  0   0   0 109   0   0   0   0   7   0   0   0   0   0   0   0   0   0]
 [  0   0   0   1  78   0   0   0   0   0   1   0   0   0   0   0   1   0]
 [  0   0   0   0   0  21   1   3   0   0   0   0   0   1   0   0   0   0]
 [  0   0   0   0   0   1  83   0   0   0   0   0   0   2   0   0   0   0]
 [  0   0   0   0   0   0   0  35   0   1   0   0   0   0   0   0   0   0]
 [  0   0   0   1   1   0   0   0  58   0   0   0   0   0   0   1   0   0]
 [  0   0   1   0   0   0   0   2   0  64   0   0   0   0   0   0   0   0]
 [  3   0   0   3   2   0   2   0   3   0 130   0   1   0   1   0   1   0]
 [  1   0   0   1   1   0   0   0   0   0   0  95   0   0   3   0   0   0]
 [  0   0   0   0   0   1   0   0   0   0   0   0 111   0   1   0   0   0]
 [  0   0   1   1   0   0   1   0   0   0   0   0   0  71   0   0   0   0]
 [  2   0   0   0   0   0   0   0   1   0   0   1   0   0  37   2   0   1]
 [  0   0   0   2   2   0   0   0   1   0   6   1   0   0   1 106   1   0]
 [  0   0   0   0   0   0   0   0   0   0   0   1   3   0   1   0  92   1]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   1   0   0  21]]

Classification Report:
              precision    recall  f1-score   support

           0       0.89      0.94      0.91        51
           1       1.00      1.00      1.00        52
           2       0.95      0.88      0.91        40
           3       0.92      0.94      0.93       116
           4       0.93      0.96      0.95        81
           5       0.91      0.81      0.86        26
           6       0.95      0.97      0.96        86
           7       0.88      0.97      0.92        36
           8       0.83      0.95      0.89        61
           9       0.97      0.96      0.96        67
          10       0.94      0.89      0.92       146
          11       0.97      0.94      0.95       101
          12       0.97      0.98      0.97       113
          13       0.91      0.96      0.93        74
          14       0.79      0.84      0.81        44
          15       0.97      0.88      0.93       120
          16       0.97      0.94      0.95        98
          17       0.91      0.95      0.93        22

    accuracy                           0.93      1334
   macro avg       0.93      0.93      0.93      1334
weighted avg       0.94      0.93      0.93      1334


Precision (Weighted): 0.9359
Recall (Weighted): 0.9340
F1-Score (Weighted): 0.9342
Test Loss: 0.2272, Test Accuracy: 0.9333
